{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from ubteacher import add_ubteacher_config\n",
    "from detectron2.config import get_cfg\n",
    "from typing import Dict, Set, List, Tuple, Iterator\n",
    "import matplotlib.pyplot as plt\n",
    "import tifffile as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plan: use qupath_annotations_latest instead of TissueAnnotator outputs for loading -- consistency across datasets and easier to use.\n",
    "1. Load only information relevant to the task at hand from json\n",
    "2. Load npy files for each image from dir, with option to create\n",
    "3. Register the dataset\n",
    "4. Idk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ubteacher.utils.train2_utils import (get_scaling, ParseFromQuPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "tissue_types = ['lymph', 'non-lymph']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image size: (33709, 39839)\n",
      "Image size: (2166, 2560)\n"
     ]
    }
   ],
   "source": [
    "original_file = '/mnt/RSX/Datasets_pathology/SRI_OSCC_lymph_labeled/Case 1 G7.svs'\n",
    "output_file = '/mnt/RSX/Datasets_pathology/GT_2023/TissueFinderV2/SRI_OSCC/Case 1 G7.npy'\n",
    "json_file = '/mnt/RSX/Datasets_pathology/SRI_OSCC_lymph_labeled/qupath_annotations_latest/Case 1 G7.json'\n",
    "\n",
    "base_dim, target_dim = get_scaling(original_file, output_file)\n",
    "\n",
    "dataset_dicts, cat_map = ParseFromQuPath(base_dim, target_dim, tissue_types).get_coco_format(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'file_name': '/mnt/RSX/Datasets_pathology/GT_2023/TissueFinderV2/SRI_OSCC/Case 1 G7.npy', 'height': 2166, 'width': 2560, 'image_id': 'Case 1 G7', 'annotations': [{'category_id': 1, 'bbox_mode': 0, 'bbox': [944, 102, 2511, 1757]}, {'category_id': 1, 'bbox_mode': 0, 'bbox': [104, 698, 1407, 2105]}]}]\n"
     ]
    }
   ],
   "source": [
    "print(dataset_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lymph': 0, 'non-lymph': 1}\n"
     ]
    }
   ],
   "source": [
    "print(cat_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lymph', 'non-lymph'}\n"
     ]
    }
   ],
   "source": [
    "print(set(cat_map.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(self, cfg, dataset_names: List[str], args: argparse.Namespace, set_seed = False):\n",
    "        \"\"\"Function to collect all input datasets and split them\n",
    "        into 'train' and 'val' sets.\n",
    "        Args:\n",
    "        dataset_names: a list of dataset names in the dataset directory\n",
    "        args: parsed command-line arguments\n",
    "        \n",
    "        \"\"\"\n",
    "        data_train = dict()\n",
    "        data_val = dict()\n",
    "        parent_dir = cfg.PARENTDIR\n",
    "\n",
    "        for name in dataset_names:\n",
    "            image_dir = os.path.join(parent_dir, name)\n",
    "            annotation_dir = os.path.join(image_dir, \"tissue_annotations\")\n",
    "\n",
    "            #Try loading from dataseed\n",
    "        try:\n",
    "            with open(cfg.DATASEED, 'r') as f:\n",
    "                data = json.load(f)\n",
    "                train_set = data['train']\n",
    "                val_set = data['val']\n",
    "        except:\n",
    "            \n",
    "            print('Dataseed did not load. Creating new seed.')\n",
    "                            \n",
    "            train_set, val_set = self.train_val_split(\n",
    "                image_dir, annotation_dir, \n",
    "            )         \n",
    "        for i, j in zip((data_train, data_val), (train_set, val_set)):\n",
    "            for k, v in j.items():\n",
    "                if k in i:\n",
    "                    i[k].extend(v)\n",
    "                else:\n",
    "                    i[k] = v\n",
    "                        \n",
    "            # Create json specifying train/val split\n",
    "        if set_seed:\n",
    "            data = {'train': train_set, 'val': val_set}\n",
    "            with open(cfg.DATASEED, 'w') as f:\n",
    "                json.dump(data, f)\n",
    "                \n",
    "        return data_train, data_val"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_path",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
